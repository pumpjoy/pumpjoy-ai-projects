{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers # Fixes the problem of HTTPS error 404 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv not found. Please check the file path.\")\n",
    "    df = pd.DataFrame() # Create an empty DataFrame to prevent errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- DataFrame Info ---\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(df[['row_id', 'body', 'rule', 'rule_violation']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Analysis of Rules (Target) and its Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Rule Analysis: Identify unique rules ---\n",
    "print(\"\\n--- Identify Unique Rules ---\")\n",
    "unique_rules = df['rule'].nunique()\n",
    "print(f\"\\n--- Rule Analysis ---\")\n",
    "print(f\"Total number of unique rules: {unique_rules}\")\n",
    "\n",
    "# Show the distinct rule names and their counts\n",
    "rule_counts = df['rule'].value_counts()\n",
    "print(\"\\nRule Counts:\")\n",
    "print(rule_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Overall Target Distribution ---\n",
    "print(\"\\n--- Target Distribution ---\")\n",
    "overall_violation_counts = df['rule_violation'].value_counts(normalize=True) * 100\n",
    "print(\"\\nOverall Rule Violation Distribution:\")\n",
    "print(overall_violation_counts)\n",
    "print(f\"Target imbalance ratio: {overall_violation_counts[0]:.2f} : {overall_violation_counts[1]:.2f}\")\n",
    "\n",
    "\n",
    "# Visualize Overall Imbalance\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='rule_violation', data=df)\n",
    "plt.title('Overall Rule Violation (Target) Distribution')\n",
    "plt.xticks([0, 1], ['No Violation (0)', 'Violation (1)'])\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Rule Violation Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Rule Violation Distribution by Rule ---\n",
    "print(\"\\n--- Rule Violation Distribution Per Rule (Imbalance Check) ---\")\n",
    "# Group by rule and calculate the mean of rule_violation (which is the violation rate)\n",
    "violation_rate_per_rule = df.groupby('rule')['rule_violation'].agg(['count', 'mean']).sort_values(by='mean', ascending=False)\n",
    "violation_rate_per_rule.columns = ['Total Samples', 'Violation Rate (Mean)']\n",
    "print(violation_rate_per_rule) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- Basically, there are only 2 rules in the whole dataset.\n",
    "- The data spread is balanced as seen from the violation count.\n",
    "- Rule distribution is also balanced, as seen from the violation rate per rule.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Analysis of Text Features (`body` and `rule`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checks for basic linguistic characteristics of the main input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Comment Body Length Analysis ---\n",
    "df['body_length'] = df['body'].apply(len)\n",
    "df['body_word_count'] = df['body'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"\\n--- Comment Body Length Statistics (in characters) ---\")\n",
    "print(df['body_length'].describe())\n",
    "\n",
    "# Visualize the length distributions for Violations vs. Non-Violations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df, x='body_length', hue='rule_violation', kde=True, bins=50)\n",
    "plt.title('Comment Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='rule_violation', y='body_length', data=df)\n",
    "plt.title('Comment Length by Violation Status')\n",
    "plt.xticks([0, 1], ['No Violation (0)', 'Violation (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- Mean of 176.8 chars - Average comment is quite long.\n",
    "- Range of 51-499 - Comments have a wide range of lengths, model should handle both long and short texts effectively.\n",
    "- Median of 138 chars - Half the comments are under 138.\n",
    "-------\n",
    "- Comments with violations tend to be long - Violating comments (orange) are generally longer than the non-violates (blue)\n",
    "- Both distributions of violate and non-violate are right-skewed (mean overestimates the most common values).\n",
    "- Violation curve (orange) is flatter and extends further; Violation comments have higher average length.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Rule Text Length Analysis ---\n",
    "df['rule_length'] = df['rule'].apply(len)\n",
    "print(\"\\n--- Rule Text Length Statistics (in characters) ---\")\n",
    "print(df['rule_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- Count of comment and rule are the same -- No nulls. Dataset is pre-cleaned.\n",
    "- Since there are only 2 distinct values (or rules), rule text is a critical, categorical feature.\n",
    "    - Model must learn the difference between \"No legal advice\" (54 char) and \"No Advertising\" (103 char).\n",
    "    - Model should learn the semantic content of the rules.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Feature combination for prompt engineering\n",
    "- Making the input rich since the dataset is small.\n",
    "- This includes **comment body**, **rule text**, and **contextual example for that rule**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the combined feature text for the Transformer input\n",
    "def remove_links_for_display(text):\n",
    "    \"\"\"\n",
    "    Temporarily removes common URLs from text for clean printing/display only.\n",
    "    \"\"\" \n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|gov|edu|co)\\b')\n",
    "    text = url_pattern.sub(r'[LINK REMOVED]', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_transformer_input(row):\n",
    "    \"\"\"\n",
    "    Creates a structured text prompt combining the comment, rule, and examples.\n",
    "    This is the input feature for RoBERTa.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Rule Context\n",
    "    rule_text = f\"RULE: {row['rule']}\"\n",
    "    \n",
    "    # 2. Positive Examples (What IS a violation)\n",
    "    pos_examples = (\n",
    "        f\"POSITIVE EXAMPLES (Violation): \"\n",
    "        f\"{row['positive_example_1']} | \"\n",
    "        f\"{row['positive_example_2']}\"\n",
    "    )\n",
    "    \n",
    "    # 3. Negative Examples (What is NOT a violation)\n",
    "    neg_examples = (\n",
    "        f\"NEGATIVE EXAMPLES (No Violation): \"\n",
    "        f\"{row['negative_example_1']} | \"\n",
    "        f\"{row['negative_example_2']}\"\n",
    "    )\n",
    "    \n",
    "    # 4. The Comment to be Classified\n",
    "    comment_body = f\"COMMENT TO CLASSIFY: {row['body']}\"\n",
    "    \n",
    "    # Combine all parts with clear separators. The Transformer will learn \n",
    "    # the relationship between these segments.\n",
    "    # The [SEP] token will be added by the tokenizer later.\n",
    "    \n",
    "    combined_text = f\"{comment_body} [SEP] {rule_text} [SEP] {pos_examples} [SEP] {neg_examples}\"\n",
    "    return combined_text\n",
    "\n",
    "# --- Application to the dataset ---\n",
    "# Apply the function to create the new input feature column\n",
    "df['model_input_text'] = df.apply(create_transformer_input, axis=1)\n",
    "\n",
    "# Display a sample input to verify the structure\n",
    "print(\"\\n--- Sample Model Input Text ---\")\n",
    "print(remove_links_for_display(df['model_input_text'].iloc[1]))\n",
    "print(\"\\n--- Length of Sample Input ---\")\n",
    "print(f\"Length of sample input: {len(df['model_input_text'].iloc[1])} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Basic text cleaning \n",
    "- Optimizes the input, removing irrelevant artifacts such as HTML or extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perform basic cleaning \n",
    "def basic_clean(text):\n",
    "    \"\"\"Minimal cleaning for Transformer input.\"\"\"\n",
    "    # Remove HTML tags (if any)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Application to the dataset ---\n",
    "# Apply cleaning to the combined text\n",
    "df['model_input_text_cleaned'] = df['model_input_text'].apply(basic_clean)\n",
    "\n",
    "# Also apply cleaning to the raw comment body, just in case\n",
    "df['body_cleaned'] = df['body'].apply(basic_clean)\n",
    "\n",
    "print(\"\\n--- Sample Cleaned Model Input Text ---\")\n",
    "print(remove_links_for_display(df['model_input_text_cleaned'].iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Preparation of dataset for training\n",
    "- X (Features) and y (Target) preparation.\n",
    "- Split the small labeled data into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df['model_input_text_cleaned']\n",
    "y = df['rule_violation']\n",
    "\n",
    "# Split the small labeled dataset into Training and Validation sets\n",
    "# Use stratification to ensure the 50/50 balance is maintained in both sets.\n",
    "# A small validation set is acceptable for the initial fine-tuning.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, # Use 20% for validation\n",
    "    random_state=42, \n",
    "    stratify=y # Stratify because the balance is perfect\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training/Validation Split Summary ---\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Training Violation Rate: {y_train.mean():.4f}\")\n",
    "print(f\"Validation Violation Rate: {y_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration --- \n",
    "BATCH_SIZE = 4 # Adjust based on your GPU memory\n",
    "ACCUMULATION_STEPS = 4 \n",
    "NUM_EPOCHS = 3\n",
    "OUTPUT_DIR = './model' # Define the target directory once\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    # 1. RoBERTa Configuration (Default, stable, no special flags)\n",
    "    \"roberta-base\": {\n",
    "        \"name\": 'roberta-base',\n",
    "        \"trust_remote_code\": False,\n",
    "        \"max_len\": 512,\n",
    "        \"special_token_handling\": None # No special handling needed\n",
    "    },\n",
    "    \n",
    "    # 2. Qwen Configuration (Requires special flags and token handling)\n",
    "    \"qwen-0.5b\": {\n",
    "        \"name\": 'Qwen/Qwen1.5-0.5B',\n",
    "        \"trust_remote_code\": True, # CRITICAL for Qwen models\n",
    "        \"max_len\": 512, # Qwen can go higher, but 512 is safe\n",
    "        \"special_token_handling\": \"pad_to_eos\" # Custom instruction\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Select the Model Here ---\n",
    "# To switch models, change this string.\n",
    "SELECTED_MODEL = \"roberta-base\" # \"roberta-base\" | \"qwen-0.5b\"\n",
    "\n",
    "CONFIG = MODEL_CONFIGS[SELECTED_MODEL]\n",
    "MODEL_NAME = CONFIG[\"name\"]\n",
    "MAX_LEN = CONFIG[\"max_len\"]\n",
    "OUTPUT_DIR = f'./final_model_acge_{SELECTED_MODEL.replace(\"/\", \"_\").lower()}'\n",
    "# -----------------------------\n",
    "\n",
    "# --- Tokenization and Encoding ---# Load the tokenizer, applying model-specific flags\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=CONFIG[\"trust_remote_code\"]\n",
    ")\n",
    "\n",
    "# Handle special token requirements (Qwen Fix)\n",
    "if CONFIG[\"special_token_handling\"] == \"pad_to_eos\" and tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Warning: Set pad_token to eos_token for {MODEL_NAME}\")\n",
    "\n",
    "def encode_data(texts, tokenizer, max_len):\n",
    "    \"\"\"Tokenizes and prepares data for the Transformer model.\"\"\"\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,      # Add [CLS] and [SEP]\n",
    "        max_length=max_len,           # Pad/truncate to MAX_LEN\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'           # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Encode Training and Validation sets\n",
    "print(\"Encoding Training Data...\")\n",
    "train_encodings = encode_data(X_train, tokenizer, MAX_LEN)\n",
    "print(\"Encoding Validation Data...\")\n",
    "val_encodings = encode_data(X_val, tokenizer, MAX_LEN)\n",
    "\n",
    "# Convert labels (y_train/y_val) to PyTorch tensors\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "# --- Custom Dataset Class ---\n",
    "class RedditCommentDataset(Dataset):\n",
    "    \"\"\"Custom Dataset to correctly package data as dictionaries for the Trainer.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Package inputs and labels into a dictionary\n",
    "        # Keys MUST match the model's forward pass arguments (input_ids, attention_mask)\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the corrected PyTorch Dataset objects\n",
    "train_dataset = RedditCommentDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditCommentDataset(val_encodings, val_labels)\n",
    "print(\"Data successfully packaged into custom PyTorch Dataset format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T04:29:30.890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Delete the log directories and temporary results from previous runs\n",
    "# This is often where many small, unnecessary files accumulate.\n",
    "\n",
    "# 1. Delete the logs directory\n",
    "if os.path.exists('./logs'):\n",
    "    shutil.rmtree('./logs')\n",
    "    print(\"Cleaned up './logs' directory.\")\n",
    "\n",
    "# 2. Delete the results directory from the Trainer\n",
    "if os.path.exists('./results_manual_eval'):\n",
    "    shutil.rmtree('./results_manual_eval')\n",
    "    print(\"Cleaned up './results_manual_eval' directory.\")\n",
    "    \n",
    "# 3. Check for the temporary prediction directory and delete\n",
    "if os.path.exists('./temp_predict'):\n",
    "    shutil.rmtree('./temp_predict')\n",
    "    print(\"Cleaned up './temp_predict' directory.\")\n",
    "\n",
    "# Now, try rerunning the trainer.train() and trainer.save_model() steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-10-14T03:49:38.447471Z",
     "shell.execute_reply": "2025-10-14T03:49:38.446399Z",
     "shell.execute_reply.started": "2025-10-14T03:44:26.293028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 05:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and tokenizer saved to: ./final_model_acge_roberta-base\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(OUTPUT_DIR):\n",
    "    # Model already exists, load it\n",
    "    print(f\"\\n✅ Model found at {OUTPUT_DIR}. Loading saved model weights.\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "    TENSORFLOW_MODE = False # Hugging Face requires this to be set if training is skipped\n",
    "    \n",
    "else:\n",
    "    # Model does not exist, initialize and train\n",
    "    print(\"\\n⏳ Saved model not found. Starting fine-tuning process.\")\n",
    "    \n",
    "    # Initialize the model from scratch\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=2,\n",
    "        trust_remote_code=CONFIG[\"trust_remote_code\"] # Dynamically applied\n",
    "    )\n",
    "    \n",
    "    # Define Training Arguments (Manual Eval)\n",
    "    TRAINING_ARGS = TrainingArguments(\n",
    "        output_dir='./results_manual_eval',\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=ACCUMULATION_STEPS, \n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"no\",             # Manual Evaluation\n",
    "        save_strategy=\"epoch\",\n",
    "        save_safetensors=False,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # Create the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TRAINING_ARGS,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Start Training\n",
    "    trainer.train()\n",
    "\n",
    "    # Save Final Model (if trained)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"\\nModel and tokenizer saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T03:49:38.448671Z",
     "iopub.status.busy": "2025-10-14T03:49:38.448402Z",
     "iopub.status.idle": "2025-10-14T03:49:46.785324Z",
     "shell.execute_reply": "2025-10-14T03:49:46.784738Z",
     "shell.execute_reply.started": "2025-10-14T03:49:38.448642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Final Manual Validation (AUC Calculation) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation AUC Score: 0.8786\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model is on the correct device for prediction\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"\\n--- Performing Final Manual Validation (AUC Calculation) ---\")\n",
    "\n",
    "# Re-initialize the Trainer here to ensure the latest model weights \n",
    "# (either loaded or just trained) are used for the prediction method.\n",
    "\n",
    "# Define minimal Training Arguments for the prediction step\n",
    "PREDICT_ARGS = TrainingArguments(\n",
    "    output_dir='./temp_predict',\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "predict_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=PREDICT_ARGS,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 1. Get predictions (logits and labels)\n",
    "predictions = predict_trainer.predict(val_dataset)\n",
    "\n",
    "# Unpack the predictions\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# 2. Calculate Probabilities and AUC\n",
    "probabilities = softmax(logits, axis=1)\n",
    "probabilities_for_auc = probabilities[:, 1] # Probability for the positive class (1)\n",
    "\n",
    "# Calculate the final AUC\n",
    "auc_score = roc_auc_score(labels, probabilities_for_auc)\n",
    "\n",
    "print(f\"\\nFinal Validation AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Clean up temporary directory\n",
    "if os.path.exists('./temp_predict'):\n",
    "    shutil.rmtree('./temp_predict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Error Analysis and Model Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Preparation of Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T03:49:46.786243Z",
     "iopub.status.busy": "2025-10-14T03:49:46.786026Z",
     "iopub.status.idle": "2025-10-14T03:49:46.802670Z",
     "shell.execute_reply": "2025-10-14T03:49:46.801796Z",
     "shell.execute_reply.started": "2025-10-14T03:49:46.786226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis DataFrame created. Error distribution:\n",
      "error_type\n",
      "Correct                0.817734\n",
      "False Positive (FP)    0.100985\n",
      "False Negative (FN)    0.081281\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'predictions' and 'labels' are available from the manual evaluation\n",
    "# and the original 'df' contains the 'body' and 'rule' columns.\n",
    "\n",
    "# 1. Convert predictions/labels to a DataFrame\n",
    "analysis_data = pd.DataFrame({\n",
    "    'true_label': labels,\n",
    "    'prob_violation': probabilities_for_auc\n",
    "})\n",
    "\n",
    "# 2. Merge with original comment data\n",
    "# Assume the order of samples in val_dataset matches the order of X_val/y_val, \n",
    "# and thus, the order of 'predictions'. Merge based on the index of X_val.\n",
    "val_indices = X_val.index\n",
    "analysis_data.index = val_indices\n",
    "analysis_df = df.loc[val_indices].copy()\n",
    "analysis_df = analysis_df.merge(analysis_data, left_index=True, right_index=True)\n",
    "\n",
    "# 3. Create the prediction column (using a simple threshold of 0.5)\n",
    "THRESHOLD = 0.5 \n",
    "analysis_df['predicted_label'] = (analysis_df['prob_violation'] >= THRESHOLD).astype(int)\n",
    "\n",
    "# 4. Create an Error Type column\n",
    "def get_error_type(row):\n",
    "    if row['true_label'] == row['predicted_label']:\n",
    "        return 'Correct'\n",
    "    elif row['true_label'] == 0 and row['predicted_label'] == 1:\n",
    "        return 'False Positive (FP)' # Model cried violation, but none existed\n",
    "    else: # true_label == 1 and predicted_label == 0\n",
    "        return 'False Negative (FN)' # Model missed a true violation\n",
    "\n",
    "analysis_df['error_type'] = analysis_df.apply(get_error_type, axis=1)\n",
    "\n",
    "print(\"Analysis DataFrame created. Error distribution:\")\n",
    "print(analysis_df['error_type'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Analysis of False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positives are comments that the model flagged as violations (Predicted=1) but were actually compliant (True=0). High-confidence FPs are the worst for user experience as they lead to unfair flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T03:49:46.804422Z",
     "iopub.status.busy": "2025-10-14T03:49:46.803490Z",
     "iopub.status.idle": "2025-10-14T03:49:46.823491Z",
     "shell.execute_reply": "2025-10-14T03:49:46.822803Z",
     "shell.execute_reply.started": "2025-10-14T03:49:46.804400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 5 False Positives (Highest Confidence) ---\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: look up  in  for sluts  your town laying on  [LINK REMOVED]\n",
      "Prob. Violation: 0.8872\n",
      "\n",
      "Rule: No legal advice: Do not offer or request legal advice.\n",
      "Comment: You should make her pay you back in alternative ways if she can't give you money. Maybe make her clean your apartment or make your lunches for you.\n",
      "\n",
      "\n",
      "And if she fails at those, you've got to lay down the law. Give her a devastating uppercut to the crotchflaps, then stomp her right in the cooter when she's down, and slap her boob for good measure. AWEAFSDFGHGH!!!\n",
      "Prob. Violation: 0.8440\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Hi! Need a way to earn extra money for food, necessities, and fun? Start earning money easily and more efficiently from your smart device or computer by generating web traffic today!  [LINK REMOVED]\n",
      "Prob. Violation: 0.8397\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: free code in 20 [LINK REMOVED]/tyrandecodefreeandshopping/\n",
      "Prob. Violation: 0.8229\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Dixie Foam is the team of Mississippi insulation contractors you need to fit any home or business with high-grade spray foam insulation in Mississippi, Alabama & Southeast Arkansas. Dixie Performance Insulation LLC 2101 Douglass Rd Macon, MS 39341-8532 662-889-1716 \n",
      "\n",
      "[LINK REMOVED]\n",
      "Prob. Violation: 0.8018\n"
     ]
    }
   ],
   "source": [
    "# Filter for High-Confidence False Positives (e.g., probability > 0.9)\n",
    "fp_df = analysis_df[analysis_df['error_type'] == 'False Positive (FP)'].sort_values(by='prob_violation', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 5 False Positives (Highest Confidence) ---\")\n",
    "for i, row in fp_df.head(5).iterrows():\n",
    "    print(f\"\\nRule: {row['rule']}\")\n",
    "    print(f\"Comment: {remove_links_for_display(row['body'])}\")\n",
    "    print(f\"Prob. Violation: {row['prob_violation']:.4f}\")\n",
    "    # print the rule and comment body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Key Questions for FP Analysis:\n",
    "1. For \"No legal advice\": Are the FPs using complex, formal language that mimics legal advice but is actually a personal opinion or a joke? (e.g., \"IANAL but...\")\n",
    "2. For \"No Advertising\": Are the FPs mentioning products/companies neutrally or in response to a direct question, without an explicit referral link or spam structure?\n",
    "\n",
    "--- Top 5 False Positives (Highest Confidence) ---\n",
    "\n",
    "Rule: No legal advice: Do not offer or request legal advice.\n",
    "Comment: If you bring in $10K or more in cash you will most likely have to fill out a form for the IRS. If you don't mind filling out the the form, deposit it all at once. Otherwise deposit it over a few weeks. You've had it accumulating over a few years, what's another month or so of holding it?\n",
    "Prob. Violation: 0.9691\n",
    "_Note: High Risk, High Similarity: This is advice on a financial/legal process. The model correctly identifies the tone and topic as aligning with prohibited advice, even if it's not a formal legal opinion._\n",
    "\n",
    "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
    "Comment: look up  in  for sluts  your town laying on  [LINK REMOVED]\n",
    "Prob. Violation: 0.9660\n",
    "_Note: Explicit Spam Structure, retains the clear linguistic pattern of spam and unsolicited content. The model correctly learns the intent of this structure is advertising/spam._\n",
    "\n",
    "Rule: No legal advice: Do not offer or request legal advice.\n",
    "Comment: hire private security to stay on the property and protect the sound set up you're going to rent and blast loud awful music at the house until they leave ... if neighbors call the authorities to make noise complaints, good then the police might take action against the squatters\n",
    "Prob. Violation: 0.9627\n",
    "_Note: Aggressive/Exaggerated Advice, the model recognizes the instructional language (\"hire,\" \"protect,\" \"blast\"). The model is focused on the directive structure of the language in a conflict scenario._\n",
    "\n",
    "Rule: No legal advice: Do not offer or request legal advice.\n",
    "Comment: Oh your fucked. Your getting 0 references from this company. Report to HR and sue the company after you kid.\n",
    "\n",
    "Bridges burned.\n",
    "Prob. Violation: 0.9579\n",
    "_Note: Legal terms found, The model looks at legal terms(\"sue the company,\" \"Report to HR\") and the call to take legal action,despite the casual tone._\n",
    "\n",
    "Rule: No legal advice: Do not offer or request legal advice.\n",
    "Comment: You should make her pay you back in alternative ways if she can't give you money. Maybe make her clean your apartment or make your lunches for you.\n",
    "\n",
    "And if she fails at those, you've got to lay down the law. Give her a devastating uppercut to the crotchflaps, then stomp her right in the cooter when she's down, and slap her boob for good measure. AWEAFSDFGHGH!!!\n",
    "Prob. Violation: 0.9518\n",
    "_Note: Mixed-Intent Advice, the first sentence is benign life advice. The second part is extreme, violent sarcasm. The model is unable to fully contextualize the extreme sarcasm to classify as non-serious._\n",
    "\n",
    "------\n",
    "Model Weakness Identification\n",
    "- The analysis confirms the model's primary weakness: Disambiguating the intent of prescriptive language.\n",
    "- Problem: The model confuses Prohibited Legal/Financial Advice with Casual Life/Financial Advice or Sarcasm/Hyperbole because both use high-signal imperative phrases (\"you should,\" \"you will,\" \"do X\").\n",
    "- Strength: The model is highly effective at identifying the structure and keywords associated with prohibited content (e.g., \"sue,\" \"IRS,\" \"advertising structure\"), which is why the AUC is high.\n",
    "---\n",
    "What can be done:\n",
    "- Semantic nuance training or Contrastive learning: Specifically train the model to differentiate 'you should consult a lawyer' (non-violation) and 'you should sue them' (violation).\n",
    "- Sarcasm/Intent modeling: A pre-trained sarcasm detection feature inthe the input pipeline to distinguish genuine advice from hyperbole.\n",
    "- Threshold adjustment: Raise the confidence threshold for automated flagging actions (0.5 to 0.95) to reduce high-confidence False Positive, which should improve user experience.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Analysis of False Negatives (FNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Negatives are true violations (True=1) that the model missed (Predicted=0). These are the worst for community health as they allow toxic/prohibited content to slip through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T03:49:46.824807Z",
     "iopub.status.busy": "2025-10-14T03:49:46.824540Z",
     "iopub.status.idle": "2025-10-14T03:49:46.840992Z",
     "shell.execute_reply": "2025-10-14T03:49:46.840272Z",
     "shell.execute_reply.started": "2025-10-14T03:49:46.824784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 5 False Negatives (Lowest Confidence) ---\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Full stream (48 min): [LINK REMOVED]\n",
      "Prob. Violation: 0.0396\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Cum Swallowing Hottie Katrina Kaif Cartoon Xvideo18+\n",
      "[LINK REMOVED]\n",
      "Prob. Violation: 0.0523\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Porn Video njoy :) [LINK REMOVED] *remove a.b*\n",
      "Prob. Violation: 0.1136\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: Nubile Films - Mia Malkova - Perfect Couple: [LINK REMOVED]\n",
      "Prob. Violation: 0.1153\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Comment: [I wanna kiss you all over! Stunning!]([LINK REMOVED]\n",
      "Prob. Violation: 0.1172\n"
     ]
    }
   ],
   "source": [
    "# Filter for Low-Confidence False Negatives (e.g., probability < 0.1)\n",
    "fn_df = analysis_df[analysis_df['error_type'] == 'False Negative (FN)'].sort_values(by='prob_violation', ascending=True)\n",
    "\n",
    "print(\"\\n--- Top 5 False Negatives (Lowest Confidence) ---\")\n",
    "for i, row in fn_df.head(5).iterrows():\n",
    "    print(f\"\\nRule: {row['rule']}\")\n",
    "    print(f\"Comment: {remove_links_for_display(row['body'])}\")\n",
    "    print(f\"Prob. Violation: {row['prob_violation']:.4f}\")\n",
    "    # print the rule and comment body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Key Questions for FN Analysis:\n",
    "\n",
    "1. Obfuscation: Are the FNs using subtle phrasing, sarcasm, or non-standard spelling to hide the violation?\n",
    "\n",
    "2. Missing Context: Are the violations very short? Sometimes, short violations are harder to catch because the model relies on the length signal that we noted in Phase 1 (violating comments are often longer).\n",
    "\n",
    "------\n",
    "Model Weakness Identification\n",
    "- All five False Negatives share a single, characteristics: They are extremely short, link-heavy, or link-only comments.\n",
    "- Feature Deficiency for Spam. The model demonstrates a catastrophic failure to detect short, unsolicited content because the preprocessing pipeline stripped the essential feature (the link presence) and the model relies too heavily on long-form semantic promotion.\n",
    "\n",
    "Fix: \n",
    "- Feature Engineering: The input feature must be changed to include a binary flag indicating the presence or absence of a URL in the original comment body.\n",
    "Example Input: [Comment] [SEP] [Rule] [SEP] [Examples] [SEP] **[URL_PRESENT: 1]**\n",
    "- A Rule-Specific Model could be made. For \"No Advertising,\" a simple, highly tuned Logistic Regression model using just a URL presence flag and term frequency of spam words might outperform the complex Transformer, demonstrating that the best solution is often a hybrid ensemble tailored to specific rule types. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D. Conclusions\n",
    "Future Improvements:\n",
    "1. Enriching Embeddings: Explore domain-specific fine-tuning or use a model like RoBERTa-large for better context understanding.\n",
    "2. Explicit Feature Engineering: If a simple feature like the presence of a URL or capitalization is a strong predictor for the \"No Advertising\" rule, add it as a non-textual feature (if you switch to a model like XGBoost, though complex with Transformers).\n",
    "3. Adversarial Training: Augment the training data with samples similar to the observed FNs to make the model more robust.\n",
    "\n",
    "\n",
    "Side note: \n",
    "The Kaggle competition where the dataset originates has lots of users using Qwen3-0.5b. Exploring its performance, obtained an expected result where RoBERTA performed better than Qwen3-0.5b. This is because RoBERTA is optimized NLU (Natural Language Understanding) where it is trained to predict masked tokens based on bidirectional context; best for task such as sequence classification. Qwen3 on the other hand is trained on the objective to perform causal LM, to predict the next token based only on the preceding tokens. This meant that the model needs to be fine-tuned for classification; require more data and more rigorous fine-tuning to rewire the model's structure for NLU. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
